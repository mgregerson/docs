---
excerpt: Monitor, debug and test the quality of your LLM outputs
---

Traceloop automatically monitors the quality of your LLM outputs. It helps you to debug and test changes to your models and prompts.

- Get real-time alerts about your modelâ€™s quality
- Execution tracing for every request
- Gradually rollout changes to models and prompts
- Debug and re-run issues from production in your IDE

Need help using Traceloop? Ping us at [dev@traceloop.com](mailto:dev@traceloop.com)

## Get Started - Install OpenLLMetry SDK

Traceloop natively plugs into OpenLLMetry SDK. To get started, pick the language you are using and follow the instructions.

<Cards>
  <Card
    title="Python"
    icon="fa-brands fa-python"
    href="/docs/openllmetry/quick-start/python"
  >
    Available
  </Card>
  <Card
    title="Javascript / Typescript"
    icon="fa-brands fa-node"
    href="/docs/openllmetry/quick-start/node-js"
  >
    Available
  </Card>
  <Card
    title="Go"
    icon="fa-brands fa-golang"
    href="/docs/openllmetry/quick-start/go"
  >
    Beta
  </Card>
  <Card
    title="Ruby"
    icon="fa-regular fa-gem"
    href="/docs/openllmetry/quick-start/ruby"
  >
    Beta
  </Card>
  <Card title="Java" icon="fa-brands fa-java">
    In Development
  </Card>
  <Card title="Elixir" icon="fa-regular fa-droplet">
    In Development
  </Card>
</Cards>
